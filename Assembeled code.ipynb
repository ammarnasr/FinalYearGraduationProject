{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "\n",
    "__C = edict()\n",
    "cfg = __C\n",
    "\n",
    "# Dataset name: flowers, birds\n",
    "__C.DATASET_NAME = 'coco'\n",
    "__C.EMBEDDING_TYPE = 'cnn-rnn'\n",
    "__C.CONFIG_NAME = ''\n",
    "__C.GPU_ID = '0'\n",
    "__C.CUDA = True\n",
    "__C.WORKERS = 6\n",
    "\n",
    "__C.NET_G = ''\n",
    "__C.NET_D = ''\n",
    "__C.STAGE1_G = ''\n",
    "__C.DATA_DIR = ''\n",
    "__C.IMG_DIR = ''\n",
    "__C.VIS_COUNT = 64\n",
    "\n",
    "__C.Z_DIM = 100\n",
    "__C.IMSIZE = 64\n",
    "__C.STAGE = 1\n",
    "\n",
    "__C.USE_LOCAL_PATHWAY = True\n",
    "__C.USE_BBOX_LAYOUT = True\n",
    "\n",
    "# Training options\n",
    "__C.TRAIN = edict()\n",
    "__C.TRAIN.FLAG = True\n",
    "__C.TRAIN.BATCH_SIZE = 64\n",
    "__C.TRAIN.MAX_EPOCH = 600\n",
    "__C.TRAIN.SNAPSHOT_INTERVAL = 50\n",
    "__C.TRAIN.PRETRAINED_MODEL = ''\n",
    "__C.TRAIN.PRETRAINED_EPOCH = 600\n",
    "__C.TRAIN.LR_DECAY_EPOCH = 600\n",
    "__C.TRAIN.DISCRIMINATOR_LR = 2e-4\n",
    "__C.TRAIN.GENERATOR_LR = 2e-4\n",
    "\n",
    "__C.TRAIN.COEFF = edict()\n",
    "__C.TRAIN.COEFF.KL = 2.0\n",
    "\n",
    "# Modal options\n",
    "__C.GAN = edict()\n",
    "__C.GAN.CONDITION_DIM = 128\n",
    "__C.GAN.DF_DIM = 64\n",
    "__C.GAN.GF_DIM = 128\n",
    "__C.GAN.R_NUM = 4\n",
    "\n",
    "__C.TEXT = edict()\n",
    "__C.TEXT.DIMENSION = 1024\n",
    "\n",
    "\n",
    "def _merge_a_into_b(a, b):\n",
    "    \"\"\"Merge config dictionary a into config dictionary b, clobbering the\n",
    "    options in b whenever they are also specified in a.\n",
    "    \"\"\"\n",
    "    if type(a) is not edict:\n",
    "        return\n",
    "\n",
    "    for k, v in a.iteritems():\n",
    "        # a must specify keys that are in b\n",
    "        if not b.has_key(k):\n",
    "            raise KeyError('{} is not a valid config key'.format(k))\n",
    "\n",
    "        # the types must match, too\n",
    "        old_type = type(b[k])\n",
    "        if old_type is not type(v):\n",
    "            if isinstance(b[k], np.ndarray):\n",
    "                v = np.array(v, dtype=b[k].dtype)\n",
    "            else:\n",
    "                raise ValueError(('Type mismatch ({} vs. {}) '\n",
    "                                  'for config key: {}').format(type(b[k]),\n",
    "                                                               type(v), k))\n",
    "\n",
    "        # recursively merge dicts\n",
    "        if type(v) is edict:\n",
    "            try:\n",
    "                _merge_a_into_b(a[k], b[k])\n",
    "            except:\n",
    "                print('Error under config key: {}'.format(k))\n",
    "                raise\n",
    "        else:\n",
    "            b[k] = v\n",
    "\n",
    "\n",
    "def cfg_from_file(filename):\n",
    "    \"\"\"Load a config file and merge it into the default options.\"\"\"\n",
    "    import yaml\n",
    "    with open(filename, 'r') as f:\n",
    "        yaml_cfg = edict(yaml.load(f))\n",
    "\n",
    "    _merge_a_into_b(yaml_cfg, __C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "import PIL\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "#from miscc.config import cfg\n",
    "\n",
    "\n",
    "class TextDataset(data.Dataset):\n",
    "    def __init__(self, data_dir, img_dir, imsize, split='train', embedding_type='cnn-rnn', transform=None, crop=True, stage=1):\n",
    "\n",
    "        self.transform = transform #Transforms are common image transformations. They can be chained together using Compose\n",
    "        self.imsize = imsize # used as target size for the crop image\n",
    "        self.crop = crop # boolean for crop the image\n",
    "        self.data = [] # not used !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.data_dir = data_dir # the all data dirctory : '../../../data/MS-COCO'\n",
    "        self.split_dir = os.path.join(data_dir, split) # the split data directory : '../../../data/MS-COCO/train'\n",
    "        self.img_dir = img_dir # the image dirctory : \"../../../data/MS-COCO/train/train2014\"\n",
    "        self.max_objects = 3 # three objects per image\n",
    "        self.stage = stage # satge 1 or 2\n",
    "\n",
    "        self.filenames = self.load_filenames() # load filenames from split dir as np array\n",
    "        self.bboxes = self.load_bboxes() # load bboxes from split dir as np array\n",
    "        self.labels = self.load_labels() # load labels from split dir as np array\n",
    "        self.embeddings = self.load_embedding(self.split_dir, embedding_type)# load embeddings from splitdir as np array\n",
    "\n",
    "    def get_img(self, img_path):\n",
    "        '''\n",
    "        return image given an img_path in PIL RGB format and apply transforms specified in the constructor\n",
    "        '''\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def load_bboxes(self):\n",
    "        '''\n",
    "        return an array of bboxes of shape : (82783, 3, 4) for the train2014 split\n",
    "        '''\n",
    "        bbox_path = os.path.join(self.split_dir, 'bboxes.pickle')\n",
    "        with open(bbox_path, \"rb\") as f:\n",
    "            bboxes = pickle.load(f)  ###############encoding=latin1!!!!!!!!!!!!!!!!!!!!!\n",
    "            bboxes = np.array(bboxes)\n",
    "        return bboxes\n",
    "\n",
    "    def load_labels(self):\n",
    "        '''\n",
    "        return an array of labels of shape : (82783, 3, 1) for the train2014 split\n",
    "        '''\n",
    "        label_path = os.path.join(self.split_dir, 'labels.pickle')\n",
    "        with open(label_path, \"rb\") as f:\n",
    "            labels = pickle.load(f)\n",
    "            labels = np.array(labels)\n",
    "        return labels\n",
    "\"\"\"\n",
    "    def load_all_captions(self):\n",
    "        caption_dict = {}\n",
    "        for key in self.filenames:\n",
    "            caption_name = '%s/text/%s.txt' % (self.data_dir, key)\n",
    "            captions = self.load_captions(caption_name)\n",
    "            caption_dict[key] = captions\n",
    "        return caption_dict\n",
    "\n",
    "    def load_captions(self, caption_name):\n",
    "        cap_path = caption_name\n",
    "        with open(cap_path, \"r\") as f:\n",
    "            captions = f.read().decode('utf8').split('\\n')\n",
    "        captions = [cap.replace(\"\\ufffd\\ufffd\", \" \") for cap in captions if len(cap) > 0]\n",
    "        return captions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    def load_embedding(self, data_dir, embedding_type):\n",
    "        '''\n",
    "        return an array of embeddings of shape : (82783, 5, 1024) for the train2014 split\n",
    "        '''\n",
    "        if embedding_type == 'cnn-rnn':\n",
    "            embedding_filename = '/char-CNN-RNN-embeddings.pickle'\n",
    "        elif embedding_type == 'cnn-gru':\n",
    "            embedding_filename = '/char-CNN-GRU-embeddings.pickle'\n",
    "        elif embedding_type == 'skip-thought':\n",
    "            embedding_filename = '/skip-thought-embeddings.pickle'\n",
    "\n",
    "        with open(data_dir + embedding_filename, 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "            embeddings = np.array(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def load_filenames(self):\n",
    "        '''\n",
    "        return a list of filenames of shape : (82783) for the train2014 split\n",
    "        '''\n",
    "        filepath = os.path.join(self.split_dir, 'filenames.pickle')\n",
    "        with open(filepath, 'rb') as f:\n",
    "            filenames = pickle.load(f)\n",
    "        print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
    "        return filenames\n",
    "\n",
    "    def crop_imgs(self, image, bbox):\n",
    "        '''\n",
    "        crops the image to a target size around the bounding box and adjsuts the bounding box\n",
    "        *check the USE_GITHUB for a sample of the resluts*\n",
    "        returns transformed image : a tensor of shape torch.Size([3, self.imsize, self.imsize])\n",
    "        bbox_scaled : a list of 1 or 2 bboxes ,depending on the stage , each of size [3,4]\n",
    "        '''\n",
    "        ori_size = image.shape[1]          #if stage=1  ori = 76  imsize=64\n",
    "        imsize = self.imsize\n",
    "\n",
    "        flip_img = random.random() < 0.5\n",
    "        img_crop = ori_size - self.imsize        #12\n",
    "        h1 = int(np.floor((img_crop) * np.random.random()))\n",
    "        w1 = int(np.floor((img_crop) * np.random.random()))\n",
    "\n",
    "        if self.stage == 1:\n",
    "            bbox_scaled = np.zeros_like(bbox)     #3,4\n",
    "            bbox_scaled[...] = -1.0\n",
    "\n",
    "            for idx in range(self.max_objects):\n",
    "                bbox_tmp = bbox[idx]\n",
    "                if bbox_tmp[0] == -1:\n",
    "                    break\n",
    "\n",
    "                x_new = max(bbox_tmp[0] * float(ori_size) - h1, 0) / float(imsize)\n",
    "                y_new = max(bbox_tmp[1] * float(ori_size) - w1, 0) / float(imsize)\n",
    "\n",
    "                width_new = min((float(ori_size)/imsize) * bbox_tmp[2], 1.0)\n",
    "                if x_new + width_new > 0.999:\n",
    "                    width_new = 1.0 - x_new - 0.001\n",
    "\n",
    "                height_new = min((float(ori_size)/imsize) * bbox_tmp[3], 1.0)\n",
    "                if y_new + height_new > 0.999:\n",
    "                    height_new = 1.0 - y_new - 0.001\n",
    "\n",
    "                if flip_img:\n",
    "                    x_new = 1.0-x_new-width_new\n",
    "\n",
    "                bbox_scaled[idx] = [x_new, y_new, width_new, height_new]\n",
    "        else:\n",
    "            # need two bboxes for stage 1 G and stage 2 G\n",
    "            bbox_scaled = [np.zeros_like(bbox), np.zeros_like(bbox)]\n",
    "            bbox_scaled[0][...] = -1.0\n",
    "            bbox_scaled[1][...] = -1.0\n",
    "\n",
    "            for idx in range(self.max_objects):\n",
    "                bbox_tmp = bbox[idx]\n",
    "                if bbox_tmp[0] == -1:\n",
    "                    break\n",
    "\n",
    "                # scale bboxes for stage 1 G\n",
    "                stage1_size = 64\n",
    "                stage1_ori_size = 76\n",
    "                x_new = max(bbox_tmp[0] * float(stage1_ori_size) - h1, 0) / float(stage1_size)\n",
    "                y_new = max(bbox_tmp[1] * float(stage1_ori_size) - w1, 0) / float(stage1_size)\n",
    "\n",
    "                width_new = min((float(stage1_ori_size) / stage1_size) * bbox_tmp[2], 1.0)\n",
    "                if x_new + width_new > 0.999:\n",
    "                    width_new = 1.0 - x_new - 0.001\n",
    "\n",
    "                height_new = min((float(stage1_ori_size) / stage1_size) * bbox_tmp[3], 1.0)\n",
    "                if y_new + height_new > 0.999:\n",
    "                    height_new = 1.0 - y_new - 0.001\n",
    "\n",
    "                if flip_img:\n",
    "                    x_new = 1.0 - x_new - width_new\n",
    "\n",
    "                bbox_scaled[0][idx] = [x_new, y_new, width_new, height_new]\n",
    "\n",
    "                # scale bboxes for stage 2 G\n",
    "                x_new = max(bbox_tmp[0] * float(ori_size) - h1, 0) / float(imsize)\n",
    "                y_new = max(bbox_tmp[1] * float(ori_size) - w1, 0) / float(imsize)\n",
    "\n",
    "                width_new = min((float(ori_size) / imsize) * bbox_tmp[2], 1.0)\n",
    "                if x_new + width_new > 0.999:\n",
    "                    width_new = 1.0 - x_new - 0.001\n",
    "\n",
    "                height_new = min((float(ori_size) / imsize) * bbox_tmp[3], 1.0)\n",
    "                if y_new + height_new > 0.999:\n",
    "                    height_new = 1.0 - y_new - 0.001\n",
    "\n",
    "                if flip_img:\n",
    "                    x_new = 1.0 - x_new - width_new\n",
    "\n",
    "                bbox_scaled[1][idx] = [x_new, y_new, width_new, height_new]\n",
    "\n",
    "\n",
    "        cropped_image = image[:, w1: w1 + imsize, h1: h1 + imsize]\n",
    "\n",
    "        if flip_img:\n",
    "            idx = [i for i in reversed(range(cropped_image.shape[2]))]\n",
    "            idx = torch.LongTensor(idx) ####################problem ; dont think it's anymore\n",
    "            transformed_image = torch.index_select(cropped_image, 2, idx)\n",
    "        else:\n",
    "            transformed_image = cropped_image\n",
    "\n",
    "        return transformed_image, bbox_scaled\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Must implemnt for pytorch dttaset classes\n",
    "        returns inputs for the model network\n",
    "        '''\n",
    "        # load image\n",
    "        key = self.filenames[index]\n",
    "        img_name = self.img_dir +\"/\" + key + \".jpg\" # The image_id and index does not crosspond 'use index of for testing'\n",
    "        img = self.get_img(img_name) #PIL image in RGB format and applied transforms\n",
    "\n",
    "        # load bbox\n",
    "        bbox = self.bboxes[index] # (82783, 3, 4) ==> (3, 4) np_array\n",
    "\n",
    "        # load label\n",
    "        label = self.labels[index] # (82783, 3, 1) ==> (3, 1) np_array\n",
    "\n",
    "        # load caption embedding\n",
    "        embeddings = self.embeddings[index, :, :]# (82783, 5, 1024) ==> ( 5, 1024)\n",
    "        embedding_ix = random.randint(0, embeddings.shape[0]-1) # random intger between 0 - 4\n",
    "        embedding = embeddings[embedding_ix, :] #( 5, 1024) ==> (1024,) np_array\n",
    "\n",
    "        if self.crop: # if true\n",
    "            img, bbox = self.crop_imgs(img, bbox)\n",
    "            # img : a tensor of shape torch.Size([3, self.imsize, self.imsize])\n",
    "            # bbox : stage1 (1, 3,4) , stage2(2 , 3,4) python lists\n",
    "\n",
    "        return img, bbox, label, embedding\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Must implemnt for pytorch dttaset classes\n",
    "        return length of - number of items in - the dataset\n",
    "        '''\n",
    "        return len(self.filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import glob\n",
    "\n",
    "from copy import deepcopy\n",
    "from miscc.config import cfg\n",
    "\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import grad\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def compute_transformation_matrix_inverse(bbox):\n",
    "    x, y = bbox[:, 0], bbox[:, 1]\n",
    "    w, h = bbox[:, 2], bbox[:, 3]\n",
    "\n",
    "    scale_x = 1.0 / w\n",
    "    scale_y = 1.0 / h\n",
    "\n",
    "    t_x = 2 * scale_x * (0.5 - (x + 0.5 * w))\n",
    "    t_y = 2 * scale_y * (0.5 - (y + 0.5 * h))\n",
    "\n",
    "    zeros = torch.cuda.FloatTensor(bbox.shape[0],1).fill_(0)\n",
    "\n",
    "    transformation_matrix = torch.cat([scale_x.unsqueeze(-1), zeros, t_x.unsqueeze(-1),\n",
    "                                       zeros, scale_y.unsqueeze(-1), t_y.unsqueeze(-1)], 1).view(-1, 2, 3)\n",
    "\n",
    "    return transformation_matrix                  #384, 2, 3\n",
    "\n",
    "\n",
    "def compute_transformation_matrix(bbox):\n",
    "    x, y = bbox[:, 0], bbox[:, 1]\n",
    "    w, h = bbox[:, 2], bbox[:, 3]\n",
    "\n",
    "    scale_x = w\n",
    "    scale_y = h\n",
    "\n",
    "    t_x = 2 * ((x + 0.5 * w) - 0.5)\n",
    "    t_y = 2 * ((y + 0.5 * h) - 0.5)\n",
    "\n",
    "    zeros = torch.cuda.FloatTensor(bbox.shape[0],1).fill_(0)\n",
    "\n",
    "    transformation_matrix = torch.cat([scale_x.unsqueeze(-1), zeros, t_x.unsqueeze(-1),\n",
    "                                       zeros, scale_y.unsqueeze(-1), t_y.unsqueeze(-1)], 1).view(-1, 2, 3)\n",
    "\n",
    "    return transformation_matrix            #384, 2, 3\n",
    "\n",
    "\n",
    "def load_validation_data(datapath, ori_size=76, imsize=64):\n",
    "\n",
    "    with open(datapath + \"bboxes.pickle\", \"rb\") as f:\n",
    "        bboxes = pickle.load(f)\n",
    "        bboxes = np.array(bboxes)\n",
    "\n",
    "    with open(datapath + \"labels.pickle\", \"rb\") as f:\n",
    "        labels = pickle.load(f)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "    return torch.from_numpy(labels), torch.from_numpy(bboxes)\n",
    "\n",
    "\n",
    "#############################\n",
    "def KL_loss(mu, logvar):\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.mean(KLD_element).mul_(-0.5)\n",
    "    return KLD\n",
    "\n",
    "\n",
    "def compute_discriminator_loss(netD, real_imgs, fake_imgs,real_labels, fake_labels,local_label,\n",
    "                               transf_matrices, transf_matrices_inv, conditions, gpus):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    batch_size = real_imgs.size(0)\n",
    "    cond = conditions.detach()\n",
    "    fake = fake_imgs.detach()\n",
    "    local_label = local_label.detach()\n",
    "    real_features = nn.parallel.data_parallel(netD, (real_imgs, local_label, transf_matrices, transf_matrices_inv), gpus)\n",
    "    fake_features = nn.parallel.data_parallel(netD, (fake, local_label, transf_matrices, transf_matrices_inv), gpus)\n",
    "    # real pairs\n",
    "    inputs = (real_features, cond)\n",
    "    real_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n",
    "    errD_real = criterion(real_logits, real_labels)\n",
    "    # wrong pairs\n",
    "    inputs = (real_features[:(batch_size-1)], cond[1:])\n",
    "    wrong_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n",
    "    errD_wrong = criterion(wrong_logits, fake_labels[1:])\n",
    "    # fake pairs\n",
    "    inputs = (fake_features, cond)\n",
    "    fake_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n",
    "    errD_fake = criterion(fake_logits, fake_labels)\n",
    "\n",
    "    if netD.get_uncond_logits is not None:\n",
    "        real_logits = nn.parallel.data_parallel(netD.get_uncond_logits, (real_features), gpus)\n",
    "        fake_logits = nn.parallel.data_parallel(netD.get_uncond_logits, (fake_features), gpus)\n",
    "        uncond_errD_real = criterion(real_logits, real_labels)\n",
    "        uncond_errD_fake = criterion(fake_logits, fake_labels)\n",
    "        #\n",
    "        errD = ((errD_real + uncond_errD_real) / 2. +\n",
    "                (errD_fake + errD_wrong + uncond_errD_fake) / 3.)\n",
    "        errD_real = (errD_real + uncond_errD_real) / 2.\n",
    "        errD_fake = (errD_fake + uncond_errD_fake) / 2.\n",
    "    else:\n",
    "        errD = errD_real + (errD_fake + errD_wrong) * 0.5\n",
    "    return errD, errD_real.item(), errD_wrong.item(), errD_fake.item()\n",
    "\n",
    "\n",
    "def compute_generator_loss(netD, fake_imgs, real_labels, local_label, transf_matrices, transf_matrices_inv, conditions, gpus):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    cond = conditions.detach()\n",
    "    fake_features = nn.parallel.data_parallel(netD, (fake_imgs, local_label, transf_matrices, transf_matrices_inv), gpus)\n",
    "    # fake pairs\n",
    "    inputs = (fake_features, cond)\n",
    "    fake_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n",
    "    errD_fake = criterion(fake_logits, real_labels)\n",
    "    if netD.get_uncond_logits is not None:\n",
    "        fake_logits = nn.parallel.data_parallel(netD.get_uncond_logits, (fake_features), gpus)\n",
    "        uncond_errD_fake = criterion(fake_logits, real_labels)\n",
    "        errD_fake += uncond_errD_fake\n",
    "    return errD_fake\n",
    "\n",
    "\n",
    "#############################\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "#############################\n",
    "def save_img_results(data_img, fake, epoch, image_dir):\n",
    "    num = cfg.VIS_COUNT\n",
    "    fake = fake[0:num]\n",
    "    # data_img is changed to [0,1]\n",
    "    if data_img is not None:\n",
    "        data_img = data_img[0:num]\n",
    "        vutils.save_image(\n",
    "            data_img, '%s/real_samples.png' % image_dir,\n",
    "            normalize=True)\n",
    "        # fake.data is still [-1, 1]\n",
    "        vutils.save_image(\n",
    "            fake.data, '%s/fake_samples_epoch_%03d.png' %\n",
    "            (image_dir, epoch), normalize=True)\n",
    "    else:\n",
    "        vutils.save_image(\n",
    "            fake.data, '%s/lr_fake_samples_epoch_%03d.png' %\n",
    "            (image_dir, epoch), normalize=True)\n",
    "\n",
    "\n",
    "def save_model(netG, netD, optimG, optimD, epoch, model_dir, saveD=False, saveOptim=False, max_to_keep=5):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'netG': netG.state_dict(),\n",
    "        'optimG': optimG.state_dict() if saveOptim else {},\n",
    "        'netD': netD.state_dict() if saveD else {},\n",
    "        'optimD': optimD.state_dict() if saveOptim else {}}\n",
    "    torch.save(checkpoint, \"{}/checkpoint_{:04}.pth\".format(model_dir, epoch))\n",
    "    print('Save G/D models')\n",
    "\n",
    "    if max_to_keep is not None and max_to_keep > 0:\n",
    "        checkpoint_list = sorted([ckpt for ckpt in glob.glob(model_dir + \"/\" + '*.pth')])\n",
    "        while len(checkpoint_list) > max_to_keep:\n",
    "            os.remove(checkpoint_list[0])\n",
    "            checkpoint_list = checkpoint_list[1:]\n",
    "\n",
    "\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "#from miscc.config import cfg\n",
    "#from miscc.utils import compute_transformation_matrix, compute_transformation_matrix_inverse\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    '''\n",
    "    3x3 convolution with padding ouptut size is the same as input size with ne w channel number equal\n",
    "    to out_planes\n",
    "    '''\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "# Upsale the spatial size by a factor of 2\n",
    "def upBlock(in_planes, out_planes):\n",
    "    '''\n",
    "    Upscale output size by factor of 2 and output channels are equal to out_planes\n",
    "    '''\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        conv3x3(in_planes, out_planes),\n",
    "        nn.BatchNorm2d(out_planes),\n",
    "        nn.ReLU(True))\n",
    "    return block\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    '''\n",
    "    Resdula block - input -> conv -> BN -> relu -> conv -> BN -> add input -> relu\n",
    "    output size and channel numbers are the same as the input\n",
    "    '''\n",
    "    def __init__(self, channel_num):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            conv3x3(channel_num, channel_num),\n",
    "            nn.BatchNorm2d(channel_num),\n",
    "            nn.ReLU(True),\n",
    "            conv3x3(channel_num, channel_num),\n",
    "            nn.BatchNorm2d(channel_num))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CA_NET(nn.Module):\n",
    "    '''\n",
    "    Takes caption embbeding of shape (1024,) as input\n",
    "    bs is batch size\n",
    "    encode        : text_embbedding(bs , 1024) -> linear (bs , self.c_dim * 2 = 256) -> relu -> output1 (bs , 256)\n",
    "                    the output is then devided to : mu= output1[: , :128](bs,128) ;logvar= output1[: , 128:](bs,128) ;\n",
    "    reparametrize : std = .5*e^(logvar) (bs,128) ; eps = random noraml tensor (bs,128)\n",
    "                    C_code  = mu + eps*std (bs , 128)\n",
    "    forward       :retuns c_code , eps ,mu\n",
    "    '''\n",
    "    # some code is modified from vae examples\n",
    "    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n",
    "    def __init__(self):\n",
    "        super(CA_NET, self).__init__()\n",
    "        self.t_dim = cfg.TEXT.DIMENSION #1024\n",
    "        self.c_dim = cfg.GAN.CONDITION_DIM #128\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 2, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def encode(self, text_embedding):\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if cfg.CUDA:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, text_embedding):\n",
    "        mu, logvar = self.encode(text_embedding)\n",
    "        c_code = self.reparametrize(mu, logvar)\n",
    "        return c_code, mu, logvar\n",
    "\n",
    "\n",
    "class D_GET_LOGITS(nn.Module):\n",
    "    '''\n",
    "    Takes for constructre:\n",
    "        number of discrimnator featurs ndf = 96 'the actcual number of cahnnels is 8*ndf =768'\n",
    "        number of condition vector features nef =128\n",
    "    Takes for the forward:\n",
    "        the image feature map from the discrimnator h_code(bs,768x4x4)\n",
    "        the conditon vector c_code (bs, 128)\n",
    "    c_code(bs, 128) -> reshape(bs, 128, 1, 1) -> repeat(bs, 128, 4, 4)\n",
    "    concat h_code&c_code(bs, 896, 4, 4) ->conv(bs , 768, 4, 4) -> BN -> relu\n",
    "    conv(bs, 1, 1, 1) -> \n",
    "    \n",
    "    return tensor vector of shape (bs,)\n",
    "    \n",
    "    'last 2 rows of table 2 and in figure 1 bottom row'\n",
    "    '''\n",
    "    def __init__(self, ndf, nef, bcondition=True):\n",
    "        super(D_GET_LOGITS, self).__init__()\n",
    "        self.df_dim = ndf   # 96 form the file : coco_s1_train.yml\n",
    "        self.ef_dim = nef   #128 will be assigned later when called in the discriminators \n",
    "        self.bcondition = bcondition\n",
    "        if bcondition: # True if the extracted  96*8x4x4 image features has been concatenated with the image caption\n",
    "            self.outlogits = nn.Sequential(\n",
    "                conv3x3(ndf * 8 + nef, ndf * 8),# from 896x4x4 ==> 768x4x4\n",
    "                nn.BatchNorm2d(ndf * 8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4))#form 768x4x4 ==>1x1x1\n",
    "                # nn.Sigmoid())\n",
    "        else:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4))#form 768x4x4 ==>1x1x1\n",
    "                # nn.Sigmoid())\n",
    "\n",
    "    def forward(self, h_code, c_code=None):\n",
    "        # conditioning output\n",
    "        if self.bcondition and c_code is not None: # check if conditon 'caption' is used and passed 'not none'\n",
    "            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
    "            c_code = c_code.repeat(1, 1, 4, 4)\n",
    "            # state size (ngf+egf) x 4 x 4\n",
    "            h_c_code = torch.cat((h_code, c_code), 1)\n",
    "        else:\n",
    "            h_c_code = h_code\n",
    "\n",
    "        output = self.outlogits(h_c_code)\n",
    "        return output.view(-1)\n",
    "\n",
    "\n",
    "def stn(image, transformation_matrix, size):\n",
    "    '''\n",
    "    Spatial Transformer network\n",
    "    given an image and transformatin matrix and an output size it applies affine transformation and samples from new grid to\n",
    "    return an image of give size\n",
    "    '''\n",
    "    grid = torch.nn.functional.affine_grid(transformation_matrix, torch.Size(size))#size is the size of the targeted output image \n",
    "    out_image = torch.nn.functional.grid_sample(image, grid)\n",
    "\n",
    "    return out_image\n",
    "\n",
    "\n",
    "class BBOX_NET(nn.Module):  # obtains the layout encoding\n",
    "    '''\n",
    "    Takes as input to forward :\n",
    "        lables : bounding box labels we concatenate the image caption embedding and the one-hot encoded bounding box\n",
    "        label and apply a dense layer with 128 units, batch normalization, and a ReLU activation to it,\n",
    "        to obtain a label ofshape(1,128)for each bounding box ==> lables (bs,3,128)\n",
    "        transf_matr_inv : for the stn\n",
    "        max_bojects : 3\n",
    "        \n",
    "    for each of the 3 lables per image -> reshape to (bs,128,1,1)->repat(bs,128,16,16)->apply stn\n",
    "    the three outputs are summed at lables_layout(bs,128,16,16)\n",
    "    label_layout->conv(bs,64,8,8)+BN+relu -> conv(bs,32,4,4)+BN+relu -> conv(bs,16,2,2)+BN+relu -> \n",
    "    reshape to (bs , 64)\n",
    "    \n",
    "    retruns layout_enocding(bs,64)\n",
    "    \n",
    "    'found at the A2 stage1 Genetraor and the global pathway G in table'\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(BBOX_NET, self).__init__()\n",
    "        self.c_dim = cfg.GAN.CONDITION_DIM  #128\n",
    "        self.encode = nn.Sequential(\n",
    "            # 128 * 16 x 16\n",
    "            conv3x3(self.c_dim, self.c_dim // 2, stride=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 64 x 8 x 8\n",
    "            conv3x3(self.c_dim // 2, self.c_dim // 4, stride=2),\n",
    "            nn.BatchNorm2d(self.c_dim // 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 32 x 4 x 4\n",
    "            conv3x3(self.c_dim // 4, self.c_dim // 8, stride=2),\n",
    "            nn.BatchNorm2d(self.c_dim // 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 16 x 2 x 2\n",
    "         )\n",
    "\n",
    "    def forward(self, labels, transf_matr_inv, max_objects):\n",
    "        label_layout = torch.cuda.FloatTensor(labels.shape[0], self.c_dim, 16, 16).fill_(0)\n",
    "        # label_layout [82783,128,16,16];\n",
    "        # labels 'One hot lables 'is of shape (82783, 3, 128)\n",
    "        for idx in range(max_objects):\n",
    "            current_label = labels[:, idx] # label idx (0,1 or 2) of each image (bs,128)\n",
    "            current_label = current_label.view(current_label.shape[0], current_label.shape[1], 1, 1)\n",
    "            #shape becomes [82783, 128, 1, 1]\n",
    "            current_label = current_label.repeat(1, 1, 16, 16)   # shape becomes [82783, 128, 16, 16]\n",
    "            current_label = stn(current_label, transf_matr_inv[:, idx], current_label.shape)\n",
    "            label_layout += current_label \n",
    "\n",
    "\n",
    "        layout_encoding = self.encode(label_layout).view(labels.shape[0], -1) #output is of shape [82783, 16*2*2=64]\n",
    "\n",
    "        return layout_encoding    #this will be concat with the noise tensor \n",
    "\n",
    "# ############# Networks for stageI GAN #############\n",
    "class STAGE1_G(nn.Module):\n",
    "    '''\n",
    "    Takes for forward :\n",
    "        text_embbeding : (bs , 1024) , use it to get c_code(bs,128)\n",
    "        noise : (bs,100)\n",
    "        transf_matrices_inv: for STN\n",
    "        label_one_hot : (bs,3,81)\n",
    "        max_objects : 3\n",
    "    Get all the inputs from above functions too feed to generator global and local pathways\n",
    "    and then use ouptut of the pathways to genertae the fake image\n",
    "    \n",
    "    returns :\n",
    "        fake image : bsx3x64x64\n",
    "        mu , logvar : bsx128\n",
    "        local_labels : bs,3,128 'bbox labels + captions applied to linear layer'\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(STAGE1_G, self).__init__()\n",
    "        self.gf_dim = cfg.GAN.GF_DIM * 8                  #192*8\n",
    "        self.ef_dim = cfg.GAN.CONDITION_DIM               #128\n",
    "        self.z_dim = cfg.Z_DIM                            #100 \n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ninput = self.z_dim + self.ef_dim                #228 noises = noise+conditioning \n",
    "        linput = self.ef_dim + 81                        #128+81= 209 encoded features + label input\n",
    "        ngf = self.gf_dim                                #192*8 = 1536\n",
    "        # TEXT.DIMENSION -> GAN.CONDITION_DIM \n",
    "        self.ca_net = CA_NET()  \n",
    "\n",
    "        if cfg.USE_BBOX_LAYOUT:\n",
    "            self.bbox_net = BBOX_NET()\n",
    "            ninput += 64                                #292 global layout+noise+conditioning \n",
    "\n",
    "        # -> ngf x 4 x 4                                # 24576\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(ninput, ngf * 4 * 4, bias=False),#292-->24576\n",
    "            nn.BatchNorm1d(ngf * 4 * 4),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        # local pathway\n",
    "        self.label = nn.Sequential(\n",
    "            nn.Linear(linput, self.ef_dim, bias=False), # 209 --> 128\n",
    "            nn.BatchNorm1d(self.ef_dim),\n",
    "            nn.ReLU(True))\n",
    "        self.local1 = upBlock(self.ef_dim, ngf // 2)  #128---->768\n",
    "        self.local2 = upBlock(ngf // 2, ngf // 4)     #768----->384\n",
    "\n",
    "        # global pathway\n",
    "        # ngf x 4 x 4 -> ngf/2 x 8 x 8\n",
    "        self.upsample1 = upBlock(ngf, ngf // 2)\n",
    "        \n",
    "        # ngf/2 x 8 x 8 -> ngf/4 x 16 x 16\n",
    "        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n",
    "        \n",
    "        # ngf/2 x 16 x 16 -> ngf/8 x 32 x 32\n",
    "        self.upsample3 = upBlock(ngf // 2, ngf // 8)\n",
    "        \n",
    "        # ngf/8 x 32 x 32-> ngf/16 x 64 x 64\n",
    "        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n",
    "        \n",
    "        # ngf/16 x 64 x 64 -> 3 x 64 x 64\n",
    "        self.img = nn.Sequential(conv3x3(ngf // 16, 3), nn.Tanh())\n",
    "\n",
    "    def forward(self, text_embedding, noise, transf_matrices_inv, label_one_hot, max_objects=3):\n",
    "        \n",
    "        c_code, mu, logvar = self.ca_net(text_embedding)\n",
    "        local_labels = torch.cuda.FloatTensor(noise.shape[0], max_objects, self.ef_dim).fill_(0)\n",
    "        #local_labels(bs,3,128)\n",
    "\n",
    "        # local(object) pathway\n",
    "        # h_code_locals is the empty canvas on which the features are added at the locations given by the bbox\n",
    "        h_code_locals = torch.cuda.FloatTensor(noise.shape[0], self.gf_dim // 4, 16, 16).fill_(0)\n",
    "        #h_code_locals(bs,384,16,16)\n",
    "        for idx in range(max_objects):\n",
    "            # generate individual label for each bounding box, based on bbox label and caption\n",
    "            current_label = self.label(torch.cat((c_code, label_one_hot[:, idx]), 1))\n",
    "            #c_code(bs,128) ; label_one_hot(bs,3,81)->idx(bs,81) -> concat(bs,209) -> label(bs,128) from above\n",
    "            local_labels[:, idx] = current_label #insert ar row idx of local_labels\n",
    "            # replicate label spatially\n",
    "            current_label = current_label.view(current_label.shape[0], self.ef_dim, 1, 1)#(bs, 128, 1,1)\n",
    "            current_label = current_label.repeat(1, 1, 4, 4)#(bs, 128, 4, 4)\n",
    "            # apply object pathway to the label to generate object features\n",
    "            h_code_local = self.local1(current_label)#bsx128x4x4---->bsx768x8x8\n",
    "            h_code_local = self.local2(h_code_local)#bsx768x8x8----->bsx384x16x16\n",
    "            # transform features to the shape of the bounding box and add to empty canvas\n",
    "            h_code_local = stn(h_code_local, transf_matrices_inv[:, idx], h_code_local.shape)\n",
    "            h_code_locals += h_code_local\n",
    "\n",
    "        # global pathway\n",
    "        if cfg.USE_BBOX_LAYOUT:\n",
    "            bbox_code = self.bbox_net(local_labels, transf_matrices_inv, max_objects) #layout_enocding(bs,64)\n",
    "            z_c_code = torch.cat((noise, c_code, bbox_code), 1)\n",
    "            #noise(bs,100) ; c_code(bs,128) ; bbox_code(bs,64) --> concat z_c_code(bs,292)\n",
    "        else:\n",
    "            z_c_code = torch.cat((noise, c_code), 1)\n",
    "        # start global pathway\n",
    "        h_code = self.fc(z_c_code)# (bs , 24576)\n",
    "        h_code = h_code.view(-1, self.gf_dim, 4, 4) #(bs , 1536 , 4 , 4)\n",
    "        h_code = self.upsample1(h_code) #(bs , 768 , 8 , 8)\n",
    "        h_code = self.upsample2(h_code) #(bs , 384 , 16, 16)\n",
    "\n",
    "        # combine local and global\n",
    "        h_code = torch.cat((h_code, h_code_locals), 1)\n",
    "        #h_code(bs , 384 , 16, 16); h_code_local (bsx384x16x16) --> cat(bsx768x16x16)\n",
    "\n",
    "        h_code = self.upsample3(h_code) #(bs , 192 , 32, 32)\n",
    "        h_code = self.upsample4(h_code) #(bs , 96  , 64, 64)\n",
    "\n",
    "        # state size 3 x 64 x 64\n",
    "        fake_img = self.img(h_code)\n",
    "        return None, fake_img, mu, logvar, local_labels\n",
    "\n",
    "\n",
    "class STAGE1_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STAGE1_D, self).__init__()\n",
    "        self.df_dim = cfg.GAN.DF_DIM     # 96\n",
    "        self.ef_dim = cfg.GAN.CONDITION_DIM    #128\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ndf, nef = self.df_dim, self.ef_dim\n",
    "\n",
    "        # local pathway\n",
    "        self.local = nn.Sequential(\n",
    "            nn.Conv2d(3 + 81, ndf * 2, 4, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, ndf, 4, 2, 1, bias=False)         #input image(3,64,64)--> output (64,32,32)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)   #---> output(128,16,16)\n",
    "        self.bn2 = nn.BatchNorm2d(ndf * 2)                          #---> output(128,16,16)\n",
    "        self.conv3 = nn.Conv2d(ndf*4, ndf * 4, 4, 2, 1, bias=False)  \n",
    "        self.bn3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(ndf*4, ndf * 8, 4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(ndf * 8)\n",
    "\n",
    "        self.get_cond_logits = D_GET_LOGITS(ndf, nef)\n",
    "        self.get_uncond_logits = None\n",
    "\n",
    "    def _encode_img(self, image, label, transf_matrices, transf_matrices_inv, max_objects):\n",
    "        # local pathway\n",
    "        # h_code_locals is the empty canvas on which the features are added at the locations given by the bbox\n",
    "        h_code_locals = torch.cuda.FloatTensor(image.shape[0], self.df_dim * 2, 16, 16).fill_(0)  #(3,128,16,16)\n",
    "        for idx in range(max_objects):\n",
    "            # get bbox label and replicate spatially\n",
    "            current_label = label[:, idx].view(label.shape[0], 81, 1, 1)\n",
    "            current_label = current_label.repeat(1, 1, 16, 16)\n",
    "            # extract features from bounding box and concatenate with the bbox label\n",
    "            h_code_local = stn(image, transf_matrices[:, idx], (image.shape[0], image.shape[1], 16, 16))\n",
    "            h_code_local = torch.cat((h_code_local, current_label), 1)\n",
    "            # apply local pathway\n",
    "            h_code_local = self.local(h_code_local)\n",
    "            # reshape extracted features to bbox layout and add to empty canvas\n",
    "            h_code_local = stn(h_code_local, transf_matrices_inv[:, idx], (h_code_local.shape[0], h_code_local.shape[1], 16, 16))\n",
    "            h_code_locals += h_code_local\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        # start global pathway\n",
    "        h_code = self.conv1(image)\n",
    "        h_code = self.act(h_code)\n",
    "        h_code = self.conv2(h_code)\n",
    "        h_code = self.bn2(h_code)\n",
    "        h_code = self.act(h_code)\n",
    "        # output global pathway(128,16,16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # combine global and local pathway\n",
    "        h_code = torch.cat((h_code, h_code_locals), 1)\n",
    "\n",
    "        h_code = self.conv3(h_code)\n",
    "        h_code = self.bn3(h_code)\n",
    "        h_code = self.act(h_code)\n",
    "\n",
    "        h_code = self.conv4(h_code)\n",
    "        h_code = self.bn4(h_code)\n",
    "        h_code = self.act(h_code)\n",
    "        return h_code\n",
    "\n",
    "    def forward(self, image, label, transf_matrices, transf_matrices_inv, max_objects=3):  # , label_one_hot):\n",
    "        img_embedding = self._encode_img(image, label, transf_matrices, transf_matrices_inv, max_objects)\n",
    "\n",
    "        return img_embedding\n",
    "\n",
    "\n",
    "# ############# Networks for stageII GAN #############\n",
    "class STAGE2_G(nn.Module):\n",
    "    def __init__(self, STAGE1_G):\n",
    "        super(STAGE2_G, self).__init__()\n",
    "        self.gf_dim = cfg.GAN.GF_DIM             #128 //192\n",
    "        self.ef_dim = cfg.GAN.CONDITION_DIM      #128\n",
    "        self.z_dim = cfg.Z_DIM                   #100\n",
    "        self.STAGE1_G = STAGE1_G\n",
    "        # fix parameters of stageI GAN\n",
    "        for param in self.STAGE1_G.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.define_module()\n",
    "\n",
    "    def _make_layer(self, block, channel_num):     \n",
    "        layers = []\n",
    "        for i in range(cfg.GAN.R_NUM):      #residual number\n",
    "            layers.append(block(channel_num))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def define_module(self):\n",
    "        ngf = self.gf_dim    #128\n",
    "        # TEXT.DIMENSION -> GAN.CONDITION_DIM\n",
    "        self.ca_net = CA_NET()\n",
    "\n",
    "        # local pathway\n",
    "        linput = self.ef_dim + 81          #128+81 =209       \n",
    "        self.label = nn.Sequential(\n",
    "            nn.Linear(linput, self.ef_dim, bias=False),\n",
    "            nn.BatchNorm1d(self.ef_dim),\n",
    "            nn.ReLU(True))\n",
    "        self.local1 = upBlock(self.ef_dim+768, ngf * 2)\n",
    "        self.local2 = upBlock(ngf * 2, ngf)\n",
    "\n",
    "        # --> 4ngf x 16 x 16\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv3x3(3, ngf),             #(192,64,64)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(ngf, ngf * 2, 4, 2, 1, bias=False),  #output image (384,32,32)\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False),  #output image (768,16,16)\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True))\n",
    "        if cfg.USE_BBOX_LAYOUT:\n",
    "            self.hr_joint = nn.Sequential(\n",
    "                conv3x3(self.ef_dim * 2 + ngf * 4, ngf * 4),   #[1, 1024, 16, 16]-->  [1, 768, 16, 16]\n",
    "                nn.BatchNorm2d(ngf * 4),\n",
    "                nn.ReLU(True))\n",
    "        else:\n",
    "            self.hr_joint = nn.Sequential(\n",
    "                conv3x3(self.ef_dim + ngf * 4, ngf * 4), \n",
    "                nn.BatchNorm2d(ngf * 4),\n",
    "                nn.ReLU(True))\n",
    "        self.residual = self._make_layer(ResBlock, ngf * 4)\n",
    "        # --> 2ngf x 32 x 32\n",
    "        self.upsample1 = upBlock(ngf * 4, ngf * 2)\n",
    "        # --> ngf x 64 x 64\n",
    "        self.upsample2 = upBlock(ngf * 2, ngf)\n",
    "        # --> ngf // 2 x 128 x 128\n",
    "        self.upsample3 = upBlock(ngf * 2, ngf // 2)\n",
    "        # --> ngf // 4 x 256 x 256\n",
    "        self.upsample4 = upBlock(ngf // 2, ngf // 4)\n",
    "        # --> 3 x 256 x 256\n",
    "        self.img = nn.Sequential(\n",
    "            conv3x3(ngf // 4, 3),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, text_embedding, noise, transf_matrices_inv,\n",
    "                transf_matrices_s2, transf_matrices_inv_s2, label_one_hot, max_objects=3):\n",
    "        _, stage1_img, _, _, _ = self.STAGE1_G(text_embedding, noise, transf_matrices_inv, label_one_hot)\n",
    "        stage1_img = stage1_img.detach()\n",
    "        encoded_img = self.encoder(stage1_img)  #(768,16,16)\n",
    "\n",
    "        # contains the generated labels of the individual bboxes\n",
    "        local_labels = torch.cuda.FloatTensor(noise.shape[0], max_objects, self.ef_dim).fill_(0) #1,3,128\n",
    "\n",
    "        c_code, mu, logvar = self.ca_net(text_embedding)\n",
    "        c_code_ = c_code.view(-1, self.ef_dim, 1, 1)\n",
    "        c_code_ = c_code_.repeat(1, 1, 16, 16) #(1,128,16,16)\n",
    "\n",
    "        if cfg.USE_BBOX_LAYOUT:\n",
    "            labels_layout = torch.cuda.FloatTensor(noise.shape[0], self.ef_dim, 16, 16).fill_(0)  #(1,128,16,16)\n",
    "            # create bbox layout by adding the bbox labels at the locations of the bbox, zeros everywhere else\n",
    "            for idx in range(max_objects):\n",
    "                # first, generate labels for each bbox, using the one-hot bbox labels and image caption\n",
    "                current_label = self.label(torch.cat((c_code, label_one_hot[:, idx]), 1)) #c_code= (1,128) ; label_one_hot (1,81)\n",
    "                #current_label=(1,128)\n",
    "                local_labels[:, idx] = current_label  # (1,3,128 ) <---- (1,128)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #*************************************#\n",
    "                # replicate label spatially\n",
    "                current_label = current_label.view(current_label.shape[0], current_label.shape[1], 1, 1)\n",
    "                current_label = current_label.repeat(1, 1, 16, 16)       #(1,128,16,16)\n",
    "                # transfer label to bbox location and add to empty canvas\n",
    "                label_local = stn(current_label, transf_matrices_inv[:, idx],\n",
    "                                  (labels_layout.shape[0], labels_layout.shape[1], 16, 16))\n",
    "                labels_layout += label_local\n",
    "            # concatenate with the other information\n",
    "            i_c_code = torch.cat([encoded_img, c_code_, labels_layout], 1)  # [1, 1024 , 16, 16]\n",
    "        else:\n",
    "            i_c_code = torch.cat([encoded_img, c_code_], 1)\n",
    "        h_code = self.hr_joint(i_c_code) #[1,768 , 16, 16]\n",
    "        h_code = self.residual(h_code)\n",
    "\n",
    "        # local pathway\n",
    "        h_code_locals = torch.cuda.FloatTensor(h_code.shape[0], self.gf_dim, 64, 64).fill_(0)  #(1, 192, 64, 64)\n",
    "        for idx in range(max_objects):\n",
    "            if not cfg.USE_BBOX_LAYOUT:\n",
    "                # generate local labels if not already done\n",
    "                current_label = self.label(torch.cat((c_code, label_one_hot[:, idx]), 1)) #current_label=(1,128)\n",
    "                local_labels[:, idx] = current_label\n",
    "                \n",
    "                \n",
    "                 #*************************************#\n",
    "            # replicate local labels spatially\n",
    "            current_label = local_labels[:, idx].view(h_code.shape[0], 128, 1, 1)\n",
    "            current_label = current_label.repeat(1, 1, 16, 16)   #(1,128,16,16)\n",
    "            # extract features from image at the location of the bbox and concat with label\n",
    "            current_patch = stn(h_code, transf_matrices_s2[:, idx], (h_code.shape[0], h_code.shape[1], 16, 16)) # (1, 786, 16, 16)\n",
    "            current_input = torch.cat((current_patch, current_label), 1)   #(1,896,16,16)  \n",
    "            \n",
    "            # apply local pathway\n",
    "            h_code_local = self.local1(current_input) # (1,384,32,32) \n",
    "            h_code_local = self.local2(h_code_local)     #(1,192,64,64) \n",
    "            # transfer features to bbox location and add to empty canvas\n",
    "            h_code_local = stn(h_code_local, transf_matrices_inv_s2[:, idx], h_code_locals.shape)\n",
    "            h_code_locals += h_code_local\n",
    "\n",
    "        # start upsampling with global pathway\n",
    "        h_code = self.upsample1(h_code)     #[1,768 , 16, 16]---> 384 x 32 x 32\n",
    "        h_code = self.upsample2(h_code)     #384 x 32 x 32 ----> 192 x 64 x 64\n",
    "\n",
    "        # combine global and local\n",
    "        h_code = torch.cat((h_code, h_code_locals), 1)   #384 x 64 x 64\n",
    "\n",
    "        h_code = self.upsample3(h_code)   #------> 96 x 128 x 128\n",
    "        h_code = self.upsample4(h_code)   #------> 48 x 256 x 256\n",
    "\n",
    "        fake_img = self.img(h_code)        #------> 3 x 256 x 256\n",
    "\n",
    "        return stage1_img, fake_img, mu, logvar, local_labels\n",
    "\n",
    "\n",
    "class STAGE2_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STAGE2_D, self).__init__()\n",
    "        self.df_dim = cfg.GAN.DF_DIM           #64\n",
    "        self.ef_dim = cfg.GAN.CONDITION_DIM    #128\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ndf, nef = self.df_dim, self.ef_dim\n",
    "##################kernal size should be 3 #########################################################\n",
    "        self.local = nn.Sequential(\n",
    "            nn.Conv2d(3 + 81, ndf * 2, 4, 1, 1, bias=False),              #(1,84,32,32)---> (1,192,31,31)\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 2, 4, 1, 1, bias=False),            #------> (1,192,30,30)\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "#########################################################################################################    \n",
    "\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, ndf, 4, 2, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(ndf*2, ndf * 4, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(ndf * 6, ndf * 8, 4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv5 = nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(ndf * 16)\n",
    "        self.conv6 = nn.Conv2d(ndf * 16, ndf * 32, 4, 2, 1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(ndf * 32)\n",
    "        self.conv7 = conv3x3(ndf * 32, ndf * 16)\n",
    "        self.bn7 = nn.BatchNorm2d(ndf * 16)\n",
    "        self.conv8 = conv3x3(ndf * 16, ndf * 8)\n",
    "        self.bn8 = nn.BatchNorm2d(ndf * 8)\n",
    "\n",
    "\n",
    "        self.get_cond_logits = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
    "        self.get_uncond_logits = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
    "\n",
    "    def _encode_img(self, image, label, transf_matrices, transf_matrices_inv, max_objects):\n",
    "        # local pathway\n",
    "        h_code_locals = torch.cuda.FloatTensor(image.shape[0], self.df_dim * 2, 32, 32).fill_(0)  #(1,128,32,32)\n",
    "        for idx in range(max_objects):\n",
    "            # get current bbox label and replicate spatially\n",
    "            current_label = label[:, idx]\n",
    "            current_label = current_label.view(label.shape[0], 81, 1, 1)\n",
    "            current_label = current_label.repeat(1, 1, 32, 32)                 #(1,81,32,32)\n",
    "            # extract features from bbox and concat with label\n",
    "            h_code_local = stn(image, transf_matrices[:, idx], (image.shape[0], image.shape[1], 32, 32))   #(1,3,32,32)\n",
    "            h_code_local = torch.cat((h_code_local, current_label), 1)                                   #(1,84,32,32)\n",
    "            \n",
    "            \n",
    "            # apply local pathway\n",
    "            h_code_local = self.local(h_code_local)\n",
    "            # transfer features to location of bbox and add to empty canvas\n",
    "            h_code_local = stn(h_code_local, transf_matrices_inv[:, idx], h_code_locals.shape)\n",
    "            h_code_locals += h_code_local        #(1, 192, 32, 32)     \n",
    "\n",
    "        # start downsampling with global pathway\n",
    "        h_code = self.conv1(image)      #(,3,256,256 )---------> (,96,128,128)\n",
    "        h_code = self.act(h_code)       #---------> (,96,128,128)\n",
    "        h_code = self.conv2(h_code)     #---------> (,192,64,64)\n",
    "        h_code = self.bn2(h_code)       #---------> (,192,64,64)\n",
    "        h_code = self.act(h_code)       #---------> (,192,64,64)\n",
    "        h_code = self.conv3(h_code)     #---------> (,384,32,32)\n",
    "        h_code = self.bn3(h_code)       #---------> (,384,32,32)      \n",
    "        h_code = self.act(h_code)       #---------> (,384,32,32)\n",
    "\n",
    "        # combine global and local\n",
    "        h_code = torch.cat((h_code, h_code_locals), 1)         #(,576,32,32)\n",
    "\n",
    "        h_code = self.conv4(h_code)       #--------> (,768,16,16)\n",
    "        h_code = self.bn4(h_code)\n",
    "        h_code = self.act(h_code)\n",
    "        h_code = self.conv5(h_code)       #--------> (,1536,8,8)\n",
    "        h_code = self.bn5(h_code)\n",
    "        h_code = self.act(h_code)\n",
    "        h_code = self.conv6(h_code)       #--------> (,3072,4,4)\n",
    "        h_code = self.bn6(h_code)\n",
    "        h_code = self.act(h_code)\n",
    "        h_code = self.conv7(h_code)       #--------> (,1536,4,4) \n",
    "        h_code = self.bn7(h_code)\n",
    "        h_code = self.act(h_code)\n",
    "        h_code = self.conv8(h_code)       #--------> (,768,4,4) \n",
    "        h_code = self.bn8(h_code)\n",
    "        h_code = self.act(h_code)\n",
    "\n",
    "        return h_code\n",
    "\n",
    "    def forward(self, image, label, transf_matrices, transf_matrices_inv, max_objects=3):\n",
    "        img_embedding = self._encode_img(image, label, transf_matrices, transf_matrices_inv, max_objects)\n",
    "\n",
    "        return img_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from six.moves import range\n",
    "from PIL import Image\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torchfile\n",
    "\n",
    "from miscc.config import cfg\n",
    "from miscc.utils import mkdir_p\n",
    "from miscc.utils import weights_init\n",
    "from miscc.utils import save_img_results, save_model\n",
    "from miscc.utils import KL_loss\n",
    "from miscc.utils import compute_discriminator_loss, compute_generator_loss\n",
    "from miscc.utils import compute_transformation_matrix, compute_transformation_matrix_inverse\n",
    "from miscc.utils import load_validation_data\n",
    "\n",
    "from tensorboard import summary\n",
    "from tensorboard import FileWriter\n",
    "\n",
    "\n",
    "class GANTrainer(object):\n",
    "    def __init__(self, output_dir):       # output_dir='../../..//output/%s_%s_%s' %\\(cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
    "\n",
    "        if cfg.TRAIN.FLAG:\n",
    "            self.model_dir = os.path.join(output_dir, 'Model')\n",
    "            self.image_dir = os.path.join(output_dir, 'Image')\n",
    "            self.log_dir = os.path.join(output_dir, 'Log')\n",
    "            mkdir_p(self.model_dir)\n",
    "            mkdir_p(self.image_dir)\n",
    "            mkdir_p(self.log_dir)\n",
    "            self.summary_writer = FileWriter(self.log_dir)   #create an event file in a given directory and add summaries and events to it\n",
    "\n",
    "        self.max_epoch = cfg.TRAIN.MAX_EPOCH                                 #120\n",
    "        self.snapshot_interval = cfg.TRAIN.SNAPSHOT_INTERVAL                 #10\n",
    "\n",
    "        s_gpus = cfg.GPU_ID.split(',')\n",
    "        self.gpus = [int(ix) for ix in s_gpus]\n",
    "        self.num_gpus = len(self.gpus)\n",
    "        self.batch_size = cfg.TRAIN.BATCH_SIZE                            # depends on the stage we are training \n",
    "        torch.cuda.set_device(self.gpus[0])\n",
    "        cudnn.benchmark = True                                        # enables benchmark mode in cudnn\n",
    "\n",
    "    # ############# For training stageI GAN #############\n",
    "    def load_network_stageI(self):\n",
    "        from model import STAGE1_G, STAGE1_D\n",
    "        netG = STAGE1_G()\n",
    "        netG.apply(weights_init)\n",
    "        print(netG)\n",
    "        netD = STAGE1_D()\n",
    "        netD.apply(weights_init)\n",
    "        print(netD)\n",
    "\n",
    "        if cfg.NET_G != '':\n",
    "            state_dict = \\\n",
    "                torch.load(cfg.NET_G, map_location=lambda storage,loc   : storage)\n",
    "            netG.load_state_dict(state_dict[\"netG\"])\n",
    "            print('Load from: ', cfg.NET_G)\n",
    "        if cfg.NET_D != '':\n",
    "            state_dict = \\\n",
    "                torch.load(cfg.NET_D,  map_location=lambda storage, loc: storage)\n",
    "            netD.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.NET_D)\n",
    "        if cfg.CUDA:\n",
    "            netG.cuda()\n",
    "            netD.cuda()\n",
    "        return netG, netD\n",
    "\n",
    "    # ############# For training stageII GAN  #############\n",
    "    def load_network_stageII(self):\n",
    "        from model import STAGE1_G, STAGE2_G, STAGE2_D\n",
    "\n",
    "        Stage1_G = STAGE1_G()\n",
    "        netG = STAGE2_G(Stage1_G)\n",
    "        netG.apply(weights_init)\n",
    "        print(netG)\n",
    "        if cfg.NET_G != '':\n",
    "            state_dict = torch.load(cfg.NET_G, map_location=lambda storage, loc: storage)\n",
    "            netG.load_state_dict(state_dict[\"netG\"])\n",
    "            print('Load from: ', cfg.NET_G)\n",
    "        elif cfg.STAGE1_G != '':\n",
    "            state_dict = torch.load(cfg.STAGE1_G, map_location=lambda storage, loc: storage)\n",
    "            netG.STAGE1_G.load_state_dict(state_dict[\"netG\"])\n",
    "            print('Load from: ', cfg.STAGE1_G)\n",
    "        else:\n",
    "            print(\"Please give the Stage1_G path\")\n",
    "            return\n",
    "\n",
    "        netD = STAGE2_D()\n",
    "        netD.apply(weights_init)\n",
    "        if cfg.NET_D != '':\n",
    "            state_dict = \\\n",
    "                torch.load(cfg.NET_D,\n",
    "                           map_location=lambda storage, loc: storage)\n",
    "            netD.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.NET_D)\n",
    "        print(netD)\n",
    "\n",
    "        if cfg.CUDA:\n",
    "            netG.cuda()\n",
    "            netD.cuda()\n",
    "        return netG, netD\n",
    "\n",
    "    def train(self, data_loader, stage=1, max_objects=3):\n",
    "        if stage == 1:       # we are checking for training of stage 1 \n",
    "            netG, netD = self.load_network_stageI()\n",
    "        else:\n",
    "            netG, netD = self.load_network_stageII()\n",
    "\n",
    "        nz = cfg.Z_DIM                                                             #100\n",
    "        batch_size = self.batch_size                                               #128\n",
    "        noise = Variable(torch.FloatTensor(batch_size, nz))                        #tensor of shape (128,100)\n",
    "        # with torch.no_grad():\n",
    "        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1), requires_grad=False)\n",
    "        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n",
    "        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n",
    "        if cfg.CUDA:\n",
    "            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "            real_labels, fake_labels = real_labels.cuda(), fake_labels.cuda()\n",
    "\n",
    "        generator_lr = cfg.TRAIN.GENERATOR_LR                                     #0.0002\n",
    "        discriminator_lr = cfg.TRAIN.DISCRIMINATOR_LR                             #0.0002\n",
    "        lr_decay_step = cfg.TRAIN.LR_DECAY_EPOCH                                  #20\n",
    "\n",
    "        netG_para = []\n",
    "        for p in netG.parameters():\n",
    "            if p.requires_grad:\n",
    "                netG_para.append(p)\n",
    "        optimizerD = optim.Adam(netD.parameters(), lr=cfg.TRAIN.DISCRIMINATOR_LR, betas=(0.5, 0.999))\n",
    "        optimizerG = optim.Adam(netG_para, lr=cfg.TRAIN.GENERATOR_LR, betas=(0.5, 0.999))\n",
    "\n",
    "        count = 0\n",
    "        for epoch in range(self.max_epoch):\n",
    "            start_t = time.time()\n",
    "            if epoch % lr_decay_step == 0 and epoch > 0:\n",
    "                generator_lr *= 0.5\n",
    "                for param_group in optimizerG.param_groups:\n",
    "                    param_group['lr'] = generator_lr\n",
    "                discriminator_lr *= 0.5\n",
    "                for param_group in optimizerD.param_groups:\n",
    "                    param_group['lr'] = discriminator_lr\n",
    "\n",
    "            for i, data in enumerate(data_loader, 0):\n",
    "                ######################################################\n",
    "                # (1) Prepare training data\n",
    "                ######################################################\n",
    "                real_img_cpu, bbox, label, txt_embedding = data\n",
    "                #real_img_cpu.shape =128, 3, 64, 64\n",
    "                #label.shape=  128, 3, 1\n",
    "                #bbox.shape=128, 3, 4\n",
    "               #txt_embedding.shape =128, 1024 \n",
    "\n",
    "                real_imgs = Variable(real_img_cpu)\n",
    "                txt_embedding = Variable(txt_embedding)\n",
    "                if cfg.CUDA:\n",
    "                    real_imgs = real_imgs.cuda()\n",
    "                    if cfg.STAGE == 1:\n",
    "                        bbox = bbox.cuda()\n",
    "                    elif cfg.STAGE == 2:\n",
    "                        bbox = [bbox[0].cuda(), bbox[1].cuda()]\n",
    "                    label = label.cuda()\n",
    "                    txt_embedding = txt_embedding.cuda()\n",
    "\n",
    "                if cfg.STAGE == 1:\n",
    "                    bbox = bbox.view(-1, 4)         # 384, 4                                                                   \n",
    "                    transf_matrices_inv = compute_transformation_matrix_inverse(bbox)                          #--->384, 2, 3\n",
    "                    transf_matrices_inv = transf_matrices_inv.view(real_imgs.shape[0], max_objects, 2, 3)      #128,3, 2, 3    \n",
    "                    transf_matrices = compute_transformation_matrix(bbox)                                      #--->384, 2, 3\n",
    "                    transf_matrices = transf_matrices.view(real_imgs.shape[0], max_objects, 2, 3)              #128,3, 2, 3 \n",
    "                elif cfg.STAGE == 2:\n",
    "                    print(bbox.shape)\n",
    "                    print(bbox[0].shape)\n",
    "                    _bbox = bbox[0].view(-1, 4) \n",
    "                    print(_bbox.shape)\n",
    "                    transf_matrices_inv = compute_transformation_matrix_inverse(_bbox)   #768,2,3\n",
    "                    transf_matrices_inv = transf_matrices_inv.view(real_imgs.shape[0], max_objects, 2, 3)\n",
    "\n",
    "                    _bbox = bbox[1].view(-1, 4)\n",
    "                    transf_matrices_inv_s2 = compute_transformation_matrix_inverse(_bbox)\n",
    "                    transf_matrices_inv_s2 = transf_matrices_inv_s2.view(real_imgs.shape[0], max_objects, 2, 3)\n",
    "                    transf_matrices_s2 = compute_transformation_matrix(_bbox)\n",
    "                    transf_matrices_s2 = transf_matrices_s2.view(real_imgs.shape[0], max_objects, 2, 3)\n",
    "\n",
    "                # produce one-hot encodings of the labels\n",
    "                _labels = label.long()\n",
    "                # remove -1 to enable one-hot converting\n",
    "                _labels[_labels < 0] = 80\n",
    "                label_one_hot = torch.cuda.FloatTensor(noise.shape[0], max_objects, 81).fill_(0)\n",
    "                label_one_hot = label_one_hot.scatter_(2, _labels, 1).float()\n",
    "\n",
    "                #######################################################\n",
    "                # (2) Generate fake images\n",
    "                ######################################################\n",
    "                noise.data.normal_(0, 1)\n",
    "                if cfg.STAGE == 1:\n",
    "                    inputs = (txt_embedding, noise, transf_matrices_inv, label_one_hot)\n",
    "                elif cfg.STAGE == 2:\n",
    "                    inputs = (txt_embedding, noise, transf_matrices_inv, transf_matrices_s2, transf_matrices_inv_s2, label_one_hot)\n",
    "                _, fake_imgs, mu, logvar, _ = nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
    "                # _, fake_imgs, mu, logvar, _ = netG(txt_embedding, noise, transf_matrices_inv, label_one_hot)\n",
    "\n",
    "                ############################\n",
    "                # (3) Update D network\n",
    "                ###########################\n",
    "                netD.zero_grad()\n",
    "\n",
    "                if cfg.STAGE == 1:\n",
    "                    errD, errD_real, errD_wrong, errD_fake = \\\n",
    "                        compute_discriminator_loss(netD, real_imgs, fake_imgs,\n",
    "                                                   real_labels, fake_labels,\n",
    "                                                   label_one_hot, transf_matrices, transf_matrices_inv,\n",
    "                                                   mu, self.gpus)\n",
    "                elif cfg.STAGE == 2:\n",
    "                    errD, errD_real, errD_wrong, errD_fake = \\\n",
    "                        compute_discriminator_loss(netD, real_imgs, fake_imgs,\n",
    "                                                   real_labels, fake_labels,\n",
    "                                                   label_one_hot, transf_matrices_s2, transf_matrices_inv_s2,\n",
    "                                                   mu, self.gpus)\n",
    "                errD.backward(retain_graph=True)\n",
    "                optimizerD.step()\n",
    "                ############################\n",
    "                # (2) Update G network\n",
    "                ###########################\n",
    "                netG.zero_grad()\n",
    "                if cfg.STAGE == 1:\n",
    "                    errG = compute_generator_loss(netD, fake_imgs,\n",
    "                                                  real_labels, label_one_hot, transf_matrices, transf_matrices_inv,\n",
    "                                                  mu, self.gpus)\n",
    "                elif cfg.STAGE == 2:\n",
    "                    errG = compute_generator_loss(netD, fake_imgs,\n",
    "                                                  real_labels, label_one_hot, transf_matrices_s2, transf_matrices_inv_s2,\n",
    "                                                  mu, self.gpus)\n",
    "                kl_loss = KL_loss(mu, logvar)\n",
    "                errG_total = errG + kl_loss * cfg.TRAIN.COEFF.KL\n",
    "                errG_total.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                count += 1\n",
    "                if i % 500 == 0:\n",
    "                    summary_D = summary.scalar('D_loss', errD.item())\n",
    "                    summary_D_r = summary.scalar('D_loss_real', errD_real)\n",
    "                    summary_D_w = summary.scalar('D_loss_wrong', errD_wrong)\n",
    "                    summary_D_f = summary.scalar('D_loss_fake', errD_fake)\n",
    "                    summary_G = summary.scalar('G_loss', errG.item())\n",
    "                    summary_KL = summary.scalar('KL_loss', kl_loss.item())\n",
    "\n",
    "                    self.summary_writer.add_summary(summary_D, count)\n",
    "                    self.summary_writer.add_summary(summary_D_r, count)\n",
    "                    self.summary_writer.add_summary(summary_D_w, count)\n",
    "                    self.summary_writer.add_summary(summary_D_f, count)\n",
    "                    self.summary_writer.add_summary(summary_G, count)\n",
    "                    self.summary_writer.add_summary(summary_KL, count)\n",
    "\n",
    "                    # save the image result for each epoch\n",
    "                    with torch.no_grad():\n",
    "                        if cfg.STAGE == 1:\n",
    "                            inputs = (txt_embedding, noise, transf_matrices_inv, label_one_hot)\n",
    "                        elif cfg.STAGE == 2:\n",
    "                            inputs = (txt_embedding, noise, transf_matrices_inv, transf_matrices_s2, transf_matrices_inv_s2, label_one_hot)\n",
    "                        lr_fake, fake, _, _, _ = nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
    "                        save_img_results(real_img_cpu, fake, epoch, self.image_dir)\n",
    "                        if lr_fake is not None:\n",
    "                            save_img_results(None, lr_fake, epoch, self.image_dir)\n",
    "            with torch.no_grad():\n",
    "                if cfg.STAGE == 1:\n",
    "                    inputs = (txt_embedding, noise, transf_matrices_inv, label_one_hot)\n",
    "                elif cfg.STAGE == 2:\n",
    "                    inputs = (txt_embedding, noise, transf_matrices_inv, transf_matrices_s2, transf_matrices_inv_s2, label_one_hot)\n",
    "                lr_fake, fake, _, _, _ = nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
    "                save_img_results(real_img_cpu, fake, epoch, self.image_dir)\n",
    "                if lr_fake is not None:\n",
    "                    save_img_results(None, lr_fake, epoch, self.image_dir)\n",
    "            end_t = time.time()\n",
    "            print('''[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f Loss_KL: %.4f\n",
    "                     Loss_real: %.4f Loss_wrong:%.4f Loss_fake %.4f\n",
    "                     Total Time: %.2fsec\n",
    "                  '''\n",
    "                  % (epoch, self.max_epoch, i, len(data_loader),\n",
    "                     errD.item(), errG.item(), kl_loss.item(),\n",
    "                     errD_real, errD_wrong, errD_fake, (end_t - start_t)))\n",
    "            if epoch % self.snapshot_interval == 0:\n",
    "                save_model(netG, netD, optimizerG, optimizerD, epoch, self.model_dir)\n",
    "        #\n",
    "        save_model(netG, netD, optimizerG, optimizerD, epoch, self.model_dir)\n",
    "        #\n",
    "        self.summary_writer.close()\n",
    "\n",
    "    def sample(self, datapath, num_samples=25, stage=1, draw_bbox=True, max_objects=3):\n",
    "        from PIL import Image, ImageDraw, ImageFont\n",
    "        import cPickle as pickle\n",
    "        import torchvision\n",
    "        import torchvision.utils as vutils\n",
    "        img_dir = cfg.IMG_DIR\n",
    "        if stage == 1:\n",
    "            netG, _ = self.load_network_stageI()\n",
    "        else:\n",
    "            netG, _ = self.load_network_stageII()\n",
    "        netG.eval()\n",
    "\n",
    "        # Load text embeddings generated from the encoder\n",
    "        t_file = torchfile.load(datapath + \"val_captions.t7\")\n",
    "        captions_list = t_file.raw_txt\n",
    "        embeddings = np.concatenate(t_file.fea_txt, axis=0)\n",
    "        num_embeddings = len(captions_list)\n",
    "        label, bbox = load_validation_data(datapath)\n",
    "\n",
    "        filepath = os.path.join(datapath, 'filenames.pickle')\n",
    "        with open(filepath, 'rb') as f:\n",
    "            filenames = pickle.load(f)\n",
    "        print('Successfully load sentences from: ', datapath)\n",
    "        print('Total number of sentences:', num_embeddings)\n",
    "        # path to save generated samples\n",
    "        save_dir = cfg.NET_G[:cfg.NET_G.find('.pth')] + \"_visualize_bbox\"\n",
    "        print(\"saving to:\", save_dir)\n",
    "        mkdir_p(save_dir)\n",
    "\n",
    "        if cfg.CUDA:\n",
    "            if cfg.STAGE == 1:\n",
    "                bbox = bbox.cuda()\n",
    "            elif cfg.STAGE == 2:\n",
    "                bbox = [bbox.clone().cuda(), bbox.cuda()]\n",
    "            label = label.cuda()\n",
    "\n",
    "        #######################################\n",
    "        if cfg.STAGE == 1:\n",
    "            bbox_ = bbox.clone()\n",
    "        elif cfg.STAGE == 2:\n",
    "            bbox_ = bbox[0].clone()\n",
    "\n",
    "        if cfg.STAGE == 1:\n",
    "            bbox = bbox.view(-1, 4)\n",
    "            transf_matrices_inv = compute_transformation_matrix_inverse(bbox)\n",
    "            transf_matrices_inv = transf_matrices_inv.view(num_embeddings, max_objects, 2, 3)\n",
    "        elif cfg.STAGE == 2:\n",
    "            _bbox = bbox[0].view(-1, 4)\n",
    "            transf_matrices_inv = compute_transformation_matrix_inverse(_bbox)\n",
    "            transf_matrices_inv = transf_matrices_inv.view(num_embeddings, max_objects, 2, 3)\n",
    "\n",
    "            _bbox = bbox[1].view(-1, 4)\n",
    "            transf_matrices_inv_s2 = compute_transformation_matrix_inverse(_bbox)\n",
    "            transf_matrices_inv_s2 = transf_matrices_inv_s2.view(num_embeddings, max_objects, 2, 3)\n",
    "            transf_matrices_s2 = compute_transformation_matrix(_bbox)\n",
    "            transf_matrices_s2 = transf_matrices_s2.view(num_embeddings, max_objects, 2, 3)\n",
    "\n",
    "        # produce one-hot encodings of the labels\n",
    "        _labels = label.long()\n",
    "        # remove -1 to enable one-hot converting\n",
    "        _labels[_labels < 0] = 80\n",
    "        label_one_hot = torch.cuda.FloatTensor(num_embeddings, max_objects, 81).fill_(0)\n",
    "        label_one_hot = label_one_hot.scatter_(2, _labels, 1).float()\n",
    "        #######################################\n",
    "\n",
    "        nz = cfg.Z_DIM\n",
    "        noise = Variable(torch.FloatTensor(9, nz))\n",
    "        if cfg.CUDA:\n",
    "            noise = noise.cuda()\n",
    "\n",
    "        imsize = 64 if stage == 1 else 256\n",
    "\n",
    "        for count in range(num_samples):\n",
    "            index = int(np.random.randint(0, num_embeddings, 1))\n",
    "            key = filenames[index]\n",
    "            img_name = img_dir + \"/\" + key + \".jpg\"\n",
    "            img = Image.open(img_name).convert('RGB').resize((imsize, imsize), Image.ANTIALIAS)\n",
    "            val_image = torchvision.transforms.functional.to_tensor(img)\n",
    "            val_image = val_image.view(1, 3, imsize, imsize)\n",
    "            val_image = (val_image - 0.5) * 2\n",
    "\n",
    "            embeddings_batch = embeddings[index]\n",
    "            transf_matrices_inv_batch = transf_matrices_inv[index]\n",
    "            label_one_hot_batch = label_one_hot[index]\n",
    "\n",
    "            embeddings_batch = np.reshape(embeddings_batch, (1, 1024)).repeat(9,0)\n",
    "            transf_matrices_inv_batch = transf_matrices_inv_batch.view(1, 3, 2, 3).repeat(9, 1, 1, 1)\n",
    "            label_one_hot_batch = label_one_hot_batch.view(1, 3, 81).repeat(9, 1, 1)\n",
    "\n",
    "            if cfg.STAGE == 2:\n",
    "                transf_matrices_s2_batch = transf_matrices_s2[index]\n",
    "                transf_matrices_s2_batch = transf_matrices_s2_batch.view(1, 3, 2, 3).repeat(9, 1, 1, 1)\n",
    "                transf_matrices_inv_s2_batch = transf_matrices_inv_s2[index]\n",
    "                transf_matrices_inv_s2_batch = transf_matrices_inv_s2_batch.view(1, 3, 2, 3).repeat(9, 1, 1, 1)\n",
    "\n",
    "            txt_embedding = Variable(torch.FloatTensor(embeddings_batch))\n",
    "            if cfg.CUDA:\n",
    "                label_one_hot_batch = label_one_hot_batch.cuda()\n",
    "                txt_embedding = txt_embedding.cuda()\n",
    "\n",
    "            #######################################################\n",
    "            # (2) Generate fake images\n",
    "            ######################################################\n",
    "            noise.data.normal_(0, 1)\n",
    "            # inputs = (txt_embedding, noise, transf_matrices_inv_batch, label_one_hot_batch)\n",
    "            if cfg.STAGE == 1:\n",
    "                inputs = (txt_embedding, noise, transf_matrices_inv_batch, label_one_hot_batch)\n",
    "            elif cfg.STAGE == 2:\n",
    "                inputs = (txt_embedding, noise, transf_matrices_inv_batch,\n",
    "                          transf_matrices_s2_batch, transf_matrices_inv_s2_batch, label_one_hot_batch)\n",
    "            with torch.no_grad():\n",
    "                _, fake_imgs, mu, logvar, _ = nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
    "\n",
    "            data_img = torch.FloatTensor(10, 3, imsize, imsize).fill_(0)\n",
    "            data_img[0] = val_image\n",
    "            data_img[1:10] = fake_imgs\n",
    "\n",
    "            if draw_bbox:\n",
    "                for idx in range(3):\n",
    "                    x, y, w, h = tuple([int(imsize*x) for x in bbox_[index, idx]])\n",
    "                    w = imsize-1 if w > imsize-1 else w\n",
    "                    h = imsize-1 if h > imsize-1 else h\n",
    "                    if x <= -1:\n",
    "                        break\n",
    "                    data_img[:10, :, y, x:x + w] = 1\n",
    "                    data_img[:10, :, y:y + h, x] = 1\n",
    "                    data_img[:10, :, y+h, x:x + w] = 1\n",
    "                    data_img[:10, :, y:y + h, x + w] = 1\n",
    "\n",
    "            vutils.save_image(data_img, '{}/{}.png'.format(save_dir, captions_list[index]), normalize=True, nrow=10)\n",
    "\n",
    "        print(\"Saved {} files to {}\".format(count+1, save_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil\n",
    "import dateutil.tz\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "dir_path = (os.path.abspath(os.path.join(os.path.realpath(__file__), './.')))  # you give it a relative path and it returns the absolute path \n",
    "sys.path.append(dir_path)\n",
    "\n",
    "from miscc.datasets import TextDataset\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "from miscc.utils import mkdir_p\n",
    "from trainer import GANTrainer\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train a GAN network')\n",
    "    parser.add_argument('--cfg', dest='cfg_file',\n",
    "                        help='optional config file',\n",
    "                        default='birds_stage1.yml', type=str)\n",
    "    parser.add_argument('--gpu',  dest='gpu_id', type=str, default='0')\n",
    "    parser.add_argument('--data_dir', dest='data_dir', type=str, default='')\n",
    "    parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    if args.cfg_file is not None:\n",
    "        cfg_from_file(args.cfg_file)\n",
    "    if args.gpu_id != -1:\n",
    "        cfg.GPU_ID = args.gpu_id\n",
    "    if args.data_dir != '':\n",
    "        cfg.DATA_DIR = args.data_dir\n",
    "    print('Using config:')\n",
    "    pprint.pprint(cfg)\n",
    "    if args.manualSeed is None:\n",
    "        args.manualSeed = random.randint(1, 10000)\n",
    "    random.seed(args.manualSeed)\n",
    "    torch.manual_seed(args.manualSeed)\n",
    "    if cfg.CUDA:\n",
    "        torch.cuda.manual_seed_all(args.manualSeed)\n",
    "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    output_dir = '../../..//output/%s_%s_%s' % \\\n",
    "                 (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    num_gpu = len(cfg.GPU_ID.split(','))\n",
    "    if cfg.TRAIN.FLAG:\n",
    "        try:\n",
    "            os.makedirs(output_dir)\n",
    "        except OSError as exc:  # Python >2.5\n",
    "            if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        copyfile(sys.argv[0], output_dir + \"/\" + sys.argv[0])\n",
    "        copyfile(\"trainer.py\", output_dir + \"/\" + \"trainer.py\")\n",
    "        copyfile(\"model.py\", output_dir + \"/\" + \"model.py\")\n",
    "        copyfile(\"miscc/utils.py\", output_dir + \"/\" + \"utils.py\")\n",
    "        copyfile(\"miscc/datasets.py\", output_dir + \"/\" + \"datasets.py\")\n",
    "        copyfile(args.cfg_file, output_dir + \"/\" + \"cfg_file.yml\")\n",
    "\n",
    "        if cfg.STAGE == 1:\n",
    "            resize = 76\n",
    "            imsize=64\n",
    "        elif cfg.STAGE == 2:\n",
    "            resize = 268\n",
    "            imsize = 256\n",
    "\n",
    "        img_transform = transforms.Compose([\n",
    "            transforms.Resize((resize, resize)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        dataset = TextDataset(cfg.DATA_DIR, cfg.IMG_DIR, split=\"train\", imsize=imsize, transform=img_transform,\n",
    "                              crop=True, stage=cfg.STAGE)\n",
    "        assert dataset\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=cfg.TRAIN.BATCH_SIZE,\n",
    "            drop_last=True, shuffle=True, num_workers=int(cfg.WORKERS))\n",
    "\n",
    "        algo = GANTrainer(output_dir)\n",
    "        algo.train(dataloader, cfg.STAGE)\n",
    "    else:\n",
    "        datapath= '%s/test/' % (cfg.DATA_DIR)\n",
    "        algo = GANTrainer(output_dir)\n",
    "        algo.sample(datapath, num_samples=25, stage=cfg.STAGE, draw_bbox=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
